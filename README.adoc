= Quarkus Observability App
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2023-05
// Metadata
:description: This application was created to showcase how to configure Logging, Metrics, and Tracing in a Quarkus and collect and manage them using the supported infrastructure of Openshift
:keywords: openshift, Quarkus, logging, metrics, tracing, red hat
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 3
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]

// Create the Table of contents here
toc::[]

== Introduction

This application was created to showcase how to configure Logging, Metrics, and Tracing in a Quarkus and collect and manage them using the supported infrastructure of Openshift.

The application was built using https://quarkus.io/[Quarkus], a Container-First framework for writing Java applications.

.Used Quarkus extensions
[cols="2*",options="header",width=100%]
|===
| Extension Name
| Purpose

| https://quarkus.io/extensions/io.quarkus/quarkus-micrometer-registry-prometheus[Micrometer Registry Prometheus]
| Expose Metrics

| https://quarkus.io/extensions/io.quarkus/quarkus-logging-json[Logging JSON]
| Format Logs in JSON

| https://quarkus.io/guides/opentelemetry[OpenTelemetry]
| Distributed Tracing

| https://quarkus.io/extensions/io.quarkus/quarkus-smallrye-health[SmallRye Health]
| Live and Running endpoints

|===

=== Openshift Components

In order to collect the logs, metrics, and traces from our application, we are going to deploy and configure several Openshift components. The installation and configuration of the components are not the focus of this repository, so I will provide links to my other repositories where I have my quickstarts for those components.

.Openshift Supported Components 
[cols="2*",options="header",width=100%]
|===
| Openshift Component
| Purpose

| https://docs.openshift.com/container-platform/4.12/distr_tracing/distr_tracing_arch/distr-tracing-architecture.html[ OpenShift distributed tracing]
| Collect and display distributed traces. It is based on https://www.jaegertracing.io/[Jaeger] and https://opentelemetry.io/[OpenTelemetry].

| https://docs.openshift.com/container-platform/4.12/monitoring/monitoring-overview.html[User Workload Monitoring]
| Collect metrics in https://github.com/OpenObservability/OpenMetrics[OpenMetrics] format from user workloads and present it in the built-in dashboard. It also allows the creation of alerts based on metrics.

| https://docs.openshift.com/container-platform/4.12/logging/cluster-logging.html[Cluster Logging Operator]
| Collect, store, and visualize logs from workloads.

|===


=== How to start?

Access the https://code.quarkus.io/?g=org.example&a=quarkus-observability-app[Code Quarkus] site that will help you to generate the application quickstart with the Quarkus extensions:

image::quarkus-quickstart.png[]

Generate the application and download it as `.zip`.


== How it works?

The application is so similar to the downloaded version, but with the following customizations:

* I've added a new endpoint to count something using the https://quarkus.io/guides/openapi-swaggerui[Swagger OpenApi library].
* I've used the https://quarkus.io/guides/micrometer[Micrometer metrics library] to generate custom metrics that I will expose in the Prometheus endpoint. I've created three new metrics:
    ** *Gauges* measure a value that can increase or decrease over time, like the speedometer on a car.
    ** *Counters* are used to measure values that only increase.
    ** *Distribution summaries* record an observed value, which will be aggregated with other recorded values and stored as a sum


== How to run it?


=== Locally


You can run your application in dev mode that enables live coding using:

[source, bash]
----
mvn compile quarkus:dev
----

> **_NOTE:_**  Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.



===  Packaging and running the application

The application can be packaged using:

[source, bash]
----
mvn package
----
It produces the `quarkus-run.jar` file in the `target/quarkus-app/` directory.
Be aware that it’s not an _über-jar_ as the dependencies are copied into the `target/quarkus-app/lib/` directory.

The application is now runnable using `java -jar target/quarkus-app/quarkus-run.jar`.

If you want to build an _über-jar_, execute the following command:

[source, bash]
----
mvn package -Dquarkus.package.type=uber-jar
----

The application, packaged as an _über-jar_, is now runnable using `java -jar target/*-runner.jar`.


=== On a Container


Manual steps to generate Container image locally:

[source, bash]
----
# Generate the Native executable
mvn package -Pnative -Dquarkus.native.container-runtime=podman -Dquarkus.native.remote-container-build=true -Dquarkus.container-image.build=true

# Add the executable to a container image
podman build -f src/main/docker/Dockerfile.native -t quarkus/quarkus-observability-app .

# Launch the application
podman run -i --rm -p 8080:8080 quarkus/quarkus-observability-app
----


== Deploy on Openshift


=== Quarkus App

Deploy the app in a new namespace using the following command:

[source, bash]
----
# Create a ConfigMap to mount in the application to configure without rebuilding
oc create configmap app-config --from-file=application.yml=src/main/resources/application-ocp.yml -n quarkus-observability

# Install the application
oc process -f openshift/quarkus-app/app.yaml | oc apply -f -

# After that, you can access the Swagger UI using the following link
oc get route app -n quarkus-observability  --template='https://{{ .spec.host }}/q/swagger-ui'
----


=== Distributed Tracing

Red Hat OpenShift distributed tracing lets you perform distributed tracing, which records the path of a request through various microservices that make up an application.

[source, bash]
----
# Install the operator
oc apply -f openshift/ocp-distributed-tracing/10-subscription.yaml

# Deploy Jaeger
oc process -f openshift/ocp-distributed-tracing/20-jaeger.yaml | oc apply -f -
----

For more information, check the https://docs.openshift.com/container-platform/4.12/distr_tracing/distr_tracing_arch/distr-tracing-architecture.html[official documentation].


=== Monitoring 

In OpenShift Container Platform 4.12, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can monitor your own projects in OpenShift Container Platform without the need for an additional monitoring solution.

[source, bash]
----
# Enable user workload monitoring
oc apply -f openshift/ocp-monitoring/10-cm-user-workload-monitoring.yaml

# Add Service Monitor to collect metrics from the App
oc process -f openshift/ocp-monitoring/20-service-monitor.yaml | oc apply -f -
----

For more information, check the https://docs.openshift.com/container-platform/4.12/monitoring/enabling-monitoring-for-user-defined-projects.html[official documentation]. 


==== Alerting

Using Openshift Metrics, it is really simple to add alerts based on those Prometheus Metrics:

[source, bash]
----
# Add Alert to monitorize requests to the API
oc process -f openshift/ocp-alerting/10-prometheus-rule.yaml | oc apply -f -
----

==== Grafana Dashboards

[source, bash]
----
# Install the Grafana Operator
oc process -f https://raw.githubusercontent.com/alvarolop/rhdg8-server/main/grafana/grafana-01-operator.yaml | oc apply -f -

# Deploy a Grafana Config
oc process -f https://raw.githubusercontent.com/alvarolop/rhdg8-server/main/grafana/grafana-02-config.yaml | oc apply -f -

# Deploy a Grafana Instance
oc process -f https://raw.githubusercontent.com/alvarolop/rhdg8-server/main/grafana/grafana-02-instance.yaml | oc apply -f -

# Configure Grafana DataSource
oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount -n grafana

oc process -f https://raw.githubusercontent.com/alvarolop/rhdg8-server/main/grafana/grafana-03-datasource.yaml \
    -p BEARER_TOKEN=$(oc get secret $(oc describe sa grafana-serviceaccount -n grafana | awk '/Tokens/{ print $2 }') -n grafana --template='{{ .data.token | base64decode }}') \
    | oc apply -f -

# Configure Grafana Dashboard for the quarkus-observability-app
oc create configmap quarkus-observability-dashboard \
    --from-file=dashboard.json=openshift/ocp-monitoring/grafana/dashboard.json -n grafana

oc process -f https://raw.githubusercontent.com/alvarolop/rhdg8-server/main/grafana/grafana-04-dashboard.yaml \
    -p DASHBOARD_NAME=quarkus-observability-dashboard \
    -p OPERATOR_NAMESPACE=grafana \
    -p CUSTOM_FOLDER_NAME="Quarkus Observability" \
    -p DASHBOARD_KEY=dashboard.json | oc apply -f -
----




=== Logging

The logging subsystem aggregates infrastructure and applications logs from throughout your cluster and stores them in a default log store. The Openshift Logging installation section consists of sections:

* Installation of the Openshift logging operator. Always needed.
* Installation of the ElasticSearch operator as the logging backend. This is mutually exclusive with section 3).
* Installation of the Loki operator as the logging backend. This is mutually exclusive with section 2).

.Logging Operator
[source, bash]
----
oc apply -f openshift/ocp-logging/00-subscription.yaml
----

==== Migration to the new Logging stack

Currently, the Openshift Logging team decided to move from EFK to Vector+Loki. The original Openshift Logging Stack was split into three products: ElasticSearch ( Log Store and Search), Fluentd (Collection and Transportation), and Kibana (Visualization). Now, there will be only two: Vector (Collection) and Loki (Store).

In order to keep up to date and age better, this repo explores both implementations.

==== Logging backend 1: ElasticSearch

[WARNING]
====
As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. As of logging version 5.6 Fluentd is deprecated and is planned to be removed in a future release.
====

.Option 1: ElasticSearch installation and deployment
[source, bash]
----
# Install the Elastic operator
oc apply -f openshift/ocp-logging/elasticsearch/10-operator.yaml

# Create the Logging instance
oc apply -f openshift/ocp-logging/elasticsearch/20-instance.yaml
----

After installing and configuring the indexing pattern, you will be able to perform queries for the logs:

.Kibana dashboard
image::kibana-dashboard.png["Kibana dashboard"]



==== Logging backend 2: Loki

.Option 2: Loki installation and deployment
[source, bash]
----
# Install the Loki operator
oc apply -f openshift/ocp-logging/loki/10-operator.yaml

# Create an AWS S3 Bucket to store the logs
./openshift/ocp-logging/loki/aws-create-bucket.sh ./aws-env-vars

# Create the Logging instance
oc process -f openshift/ocp-logging/loki/20-instance.yaml \
    --param-file aws-env-vars --ignore-unknown-parameters=true | oc apply -f -

# Enable the console plugin
# -> This plugin adds the logging view into the 'observe' menu in the OpenShift console. It requires OpenShift 4.10.
oc patch console.operator cluster --type json -p '[{"op": "add", "path": "/spec/plugins", "value": ["logging-view-plugin"]}]'
----

.Loki dashboard
image::loki-dashboard.png["Loki dashboard"]


